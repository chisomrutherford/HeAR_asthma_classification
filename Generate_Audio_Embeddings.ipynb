{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/0RNhqqDHEGl+waqd0eSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chisomrutherford/HeAR_asthma_classification/blob/main/Generate_Audio_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZhxMP0t822t"
      },
      "outputs": [],
      "source": [
        "# mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download SPRS repo to Google Drive\n",
        "%cd /content/drive/MyDrive/\n",
        "!git clone https://github.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound.git"
      ],
      "metadata": {
        "id": "pB2UE6SI88Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "\n",
        "#from transformers import AutoProcessor, TFAutoModel\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vqrxkZowF52H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "iICN41luHuo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Google's HeAR model for generating embeddings\n",
        "\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "model = from_pretrained_keras(\"google/hear\")"
      ],
      "metadata": {
        "id": "R9WSa-q4IQIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import .wav and .json folders from Google Drive\n",
        "\n",
        "TRAIN_WAV_DIR = \"/content/drive/MyDrive/SPRSound/Classification/train_classification_wav\"\n",
        "TRAIN_JSON_DIR = \"/content/drive/MyDrive/SPRSound/Classification/train_classification_json\"\n",
        "\n",
        "VALID_WAV_DIR = \"/content/drive/MyDrive/SPRSound/Classification/valid_classification_wav\"\n",
        "VALID_JSON_DIR = \"/content/drive/MyDrive/SPRSound/Classification/valid_classification_json\"\n",
        "\n",
        "TRAIN_OUT_DIR = \"/content/asthma_clips/train\"\n",
        "VALID_OUT_DIR = \"/content/asthma_clips/valid\"\n",
        "\n",
        "os.makedirs(TRAIN_OUT_DIR, exist_ok=True)\n",
        "os.makedirs(VALID_OUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "3z8eUDFvA75n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "included_labels = ['Normal', 'Wheeze', 'Wheeze+Crackle', 'Rhonchi', 'Stridor']\n",
        "\n",
        "def process_record(wav_path, json_path, output_dir):\n",
        "    \"\"\"\n",
        "    Processes a respiratory sound recording into 2-second audio clips based on event-level annotations.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    wav_path : str\n",
        "        Path to the input `.wav` file (original full-length respiratory sound).\n",
        "\n",
        "    json_path : str\n",
        "        Path to the corresponding `.json` file containing event-level annotations.\n",
        "\n",
        "    output_dir : str\n",
        "        Directory where the extracted and standardized audio clips will be saved.\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    List[Tuple[str, int, str]]\n",
        "        A list of tuples, each containing:\n",
        "            - the saved clip filename,\n",
        "            - the binary label (0 = Normal, 1 = Abnormal),\n",
        "            - the original event label (e.g., \"Wheeze\", \"Rhonchi\").\n",
        "\n",
        "    Description:\n",
        "    -----------\n",
        "    - The function loads the `.wav` file and reads its event-level annotations from the `.json` file.\n",
        "    - Only medically meaningful labels are included (as defined in `included_labels`).\n",
        "    - Recordings labeled as \"Poor Quality\" are skipped entirely.\n",
        "    - For each valid event, the audio segment is:\n",
        "        1. Extracted based on start and end timestamps (in milliseconds),\n",
        "        2. Resampled to 16kHz mono,\n",
        "        3. Standardized to 2 seconds via truncation or zero-padding,\n",
        "        4. Saved as a new `.wav` clip in the output directory.\n",
        "    - The filename format is: `<original_id>_<event_index>_<binary_label>.wav`\n",
        "    \"\"\"\n",
        "    y, _ = librosa.load(wav_path, sr=sr, mono=True)\n",
        "\n",
        "    with open(json_path) as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    # Skip if recording is marked as \"Poor Quality\"\n",
        "    if meta.get(\"recording_annotation\") == \"Poor Quality\":\n",
        "        return []\n",
        "\n",
        "    events = meta.get(\"event_annotation\", [])\n",
        "    base = os.path.splitext(os.path.basename(wav_path))[0]\n",
        "    saved = []\n",
        "\n",
        "    for idx, event in enumerate(events):\n",
        "        label = event['type']\n",
        "        if label not in included_labels:\n",
        "            continue\n",
        "\n",
        "        # Convert start/end from ms to samples\n",
        "        start_sample = int((float(event['start']) / 1000.0) * sr)\n",
        "        end_sample = int((float(event['end']) / 1000.0) * sr)\n",
        "        segment = y[start_sample:end_sample]\n",
        "\n",
        "        # Standardize length to exactly 2 seconds\n",
        "        if len(segment) < clip_len:\n",
        "            segment = librosa.util.fix_length(segment, size=clip_len)\n",
        "        else:\n",
        "            segment = segment[:clip_len]\n",
        "\n",
        "        # Binary classification: 0 = Normal, 1 = Abnormal\n",
        "        binary_label = 0 if label == \"Normal\" else 1\n",
        "\n",
        "        # Save the clip\n",
        "        outname = f\"{base}_{idx}_{binary_label}.wav\"\n",
        "        outpath = os.path.join(output_dir, outname)\n",
        "        sf.write(outpath, segment, sr)\n",
        "\n",
        "        saved.append((outname, binary_label, label))\n",
        "\n",
        "    return saved\n",
        "\n",
        "print('Function Called')\n"
      ],
      "metadata": {
        "id": "vXB1otCmNQft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def batch_process(wav_dir, json_dir, out_dir, metadata_path):\n",
        "    \"\"\"\n",
        "    Processes a batch of respiratory sound recordings and their corresponding JSON annotations.\n",
        "\n",
        "    For each `.wav` file in `wav_dir`, this function:\n",
        "      - Finds the corresponding `.json` annotation in `json_dir`\n",
        "      - Extracts and saves 2-second labeled audio segments to `out_dir`\n",
        "      - Collects metadata about each segment (filename, binary label, original label)\n",
        "\n",
        "    Finally, it saves all metadata to a CSV at `metadata_path`.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    wav_dir : str\n",
        "        Path to directory containing .wav audio files.\n",
        "\n",
        "    json_dir : str\n",
        "        Path to directory containing .json annotation files (same base names as .wav files).\n",
        "\n",
        "    out_dir : str\n",
        "        Directory where processed 2-second clips will be saved.\n",
        "\n",
        "    metadata_path : str\n",
        "        Path to the CSV file where metadata for all saved clips will be written.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        A DataFrame containing metadata for each saved audio segment, with columns:\n",
        "        [\"filename\", \"binary_label\", \"original_label\"]\n",
        "    \"\"\"\n",
        "\n",
        "    all_metadata = []  # List to store metadata for all processed clips\n",
        "\n",
        "    # Loop through each .wav file in the audio directory\n",
        "    for f in os.listdir(wav_dir):\n",
        "        if f.endswith(\".wav\"):\n",
        "            wav_path = os.path.join(wav_dir, f)\n",
        "            json_path = os.path.join(json_dir, f.replace(\".wav\", \".json\"))\n",
        "\n",
        "            # Process only if a corresponding .json annotation file exists\n",
        "            if os.path.exists(json_path):\n",
        "                # Process the .wav and .json pair to extract valid segments\n",
        "                segments = process_record(wav_path, json_path, out_dir)\n",
        "                all_metadata.extend(segments)  # Add all returned segments to the metadata list\n",
        "\n",
        "    # Create a DataFrame from the collected metadata\n",
        "    df = pd.DataFrame(all_metadata, columns=[\"filename\", \"binary_label\", \"original_label\"])\n",
        "\n",
        "    # Save metadata as a CSV file\n",
        "    df.to_csv(metadata_path, index=False)\n",
        "\n",
        "    print(f\"Done. Saved {len(df)} clips to {out_dir}\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "SH7gKFAxQPit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set directory paths\n",
        "train_wav_dir = \"/content/drive/MyDrive/SPRSound/Classification/train_classification_wav\"\n",
        "train_json_dir = \"/content/drive/MyDrive/SPRSound/Classification/train_classification_json\"\n",
        "train_out_dir = \"/content/asthma_clips/train\"\n",
        "train_metadata_csv = \"/content/asthma_clips/train_metadata.csv\"\n",
        "\n",
        "valid_wav_dir = \"/content/drive/MyDrive/SPRSound/Classification/valid_classification_wav/2022\"\n",
        "valid_json_dir = \"/content/drive/MyDrive/SPRSound/Classification/valid_classification_json/2022/intra_test_json\"\n",
        "valid_out_dir = \"/content/asthma_clips/valid\"\n",
        "valid_metadata_csv = \"/content/asthma_clips/valid_metadata.csv\"\n",
        "\n",
        "# Make sure output folders exist\n",
        "os.makedirs(train_out_dir, exist_ok=True)\n",
        "os.makedirs(valid_out_dir, exist_ok=True)\n",
        "\n",
        "# Process both sets\n",
        "#train_df = batch_process(train_wav_dir, train_json_dir, train_out_dir, train_metadata_csv)\n",
        "#valid_df = batch_process(valid_wav_dir, valid_json_dir, valid_out_dir, valid_metadata_csv)\n"
      ],
      "metadata": {
        "id": "kFZYpwTAQxK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def batch_process(wav_dir, json_dir, out_dir, metadata_path):\n",
        "    \"\"\"\n",
        "    Batch processes respiratory sound recordings and their annotations.\n",
        "\n",
        "    For each `.wav` file in `wav_dir`, the function:\n",
        "      - Locates the corresponding `.json` annotation in `json_dir`\n",
        "      - Uses `process_record()` to extract 2-second labeled audio segments\n",
        "      - Saves each segment to `out_dir`\n",
        "      - Records metadata (filename, binary label, and original label)\n",
        "\n",
        "    A progress bar is displayed during processing, and a metadata CSV is written to `metadata_path`.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    wav_dir : str\n",
        "        Directory containing .wav audio files.\n",
        "\n",
        "    json_dir : str\n",
        "        Directory containing corresponding .json annotation files.\n",
        "\n",
        "    out_dir : str\n",
        "        Directory to save the processed 2-second audio clips.\n",
        "\n",
        "    metadata_path : str\n",
        "        File path to save the resulting metadata CSV.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame containing metadata with columns:\n",
        "        [\"filename\", \"binary_label\", \"original_label\"]\n",
        "    \"\"\"\n",
        "    all_metadata = []  # List to collect metadata for all audio segments\n",
        "\n",
        "    # Get list of all .wav files (non-recursive)\n",
        "    wav_files = [f for f in os.listdir(wav_dir) if f.endswith(\".wav\")]\n",
        "\n",
        "    # Loop through each .wav file with a progress bar\n",
        "    for f in tqdm(wav_files, desc=f\"Processing {os.path.basename(out_dir)}\", unit=\"file\"):\n",
        "        wav_path = os.path.join(wav_dir, f)\n",
        "        json_path = os.path.join(json_dir, f.replace(\".wav\", \".json\"))\n",
        "\n",
        "        # Proceed only if the corresponding .json file exists\n",
        "        if os.path.exists(json_path):\n",
        "            # Extract segments using process_record\n",
        "            segments = process_record(wav_path, json_path, out_dir)\n",
        "            all_metadata.extend(segments)\n",
        "\n",
        "    # Create a DataFrame from collected metadata\n",
        "    df = pd.DataFrame(all_metadata, columns=[\"filename\", \"binary_label\", \"original_label\"])\n",
        "\n",
        "    # Save metadata to CSV\n",
        "    df.to_csv(metadata_path, index=False)\n",
        "\n",
        "    print(f\"Done. Saved {len(df)} clips to {out_dir}\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "OrIpzEDUer38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def batch_process_recursive(wav_dir, json_dir, out_dir, metadata_path):\n",
        "    \"\"\"\n",
        "    Recursively processes respiratory audio files and corresponding annotation files,\n",
        "    extracting 2-second labeled clips using `process_record()` and saving them with metadata.\n",
        "\n",
        "    This function:\n",
        "      - Walks through all subdirectories in `wav_dir` to find `.wav` files.\n",
        "      - For each `.wav`, computes the expected relative path to its `.json` annotation in `json_dir`.\n",
        "      - Uses `process_record()` to extract and save labeled audio segments into `out_dir`.\n",
        "      - Records metadata (filename, binary label, original label) for each saved clip.\n",
        "      - Saves the metadata to a CSV at `metadata_path`.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    wav_dir : str\n",
        "        Root directory containing .wav audio files (can include nested folders).\n",
        "\n",
        "    json_dir : str\n",
        "        Root directory containing corresponding .json annotation files (same folder structure as wav_dir).\n",
        "\n",
        "    out_dir : str\n",
        "        Directory to save the extracted 2-second audio clips.\n",
        "\n",
        "    metadata_path : str\n",
        "        Path to save the resulting metadata CSV file.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        A DataFrame containing metadata for all saved audio segments, with columns:\n",
        "        [\"filename\", \"binary_label\", \"original_label\"]\n",
        "    \"\"\"\n",
        "\n",
        "    all_metadata = []  # To collect metadata from all processed segments\n",
        "    all_wav_paths = []  # To collect all .wav files recursively\n",
        "\n",
        "    # Step 1: Walk through wav_dir recursively and collect all .wav file paths\n",
        "    for root, _, files in os.walk(wav_dir):\n",
        "        for f in files:\n",
        "            if f.endswith(\".wav\"):\n",
        "                all_wav_paths.append(os.path.join(root, f))\n",
        "\n",
        "    # Step 2: Process each wav file using a progress bar\n",
        "    for wav_path in tqdm(all_wav_paths, desc=f\"Processing {os.path.basename(out_dir)}\", unit=\"file\"):\n",
        "        # Calculate the relative path from wav_dir to the current .wav\n",
        "        # and derive the corresponding json path\n",
        "        rel_path = os.path.relpath(wav_path, wav_dir).replace(\".wav\", \".json\")\n",
        "        json_path = os.path.join(json_dir, rel_path)\n",
        "\n",
        "        # Step 3: Process the file if the corresponding JSON exists\n",
        "        if os.path.exists(json_path):\n",
        "            segments = process_record(wav_path, json_path, out_dir)\n",
        "            all_metadata.extend(segments)\n",
        "\n",
        "    # Step 4: Save all metadata to CSV\n",
        "    df = pd.DataFrame(all_metadata, columns=[\"filename\", \"binary_label\", \"original_label\"])\n",
        "    df.to_csv(metadata_path, index=False)\n",
        "\n",
        "    print(f\"Done. Saved {len(df)} clips to {out_dir}\")\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "g84zYijRex9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = batch_process(\n",
        "    wav_dir=train_wav_dir,\n",
        "    json_dir=train_json_dir,\n",
        "    out_dir=train_out_dir,\n",
        "    metadata_path=train_metadata_csv\n",
        ")"
      ],
      "metadata": {
        "id": "5lVfg177mEQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df = batch_process_recursive(\n",
        "    wav_dir=\"/content/drive/MyDrive/SPRSound/Classification/valid_classification_wav/2022\",\n",
        "    json_dir=\"/content/drive/MyDrive/SPRSound/Classification/valid_classification_json/2022/intra_test_json\",\n",
        "    out_dir=\"/content/asthma_clips/valid\",\n",
        "    metadata_path=\"/content/asthma_clips/valid_metadata.csv\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "FK_ri5p3fksF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df = pd.read_csv(\"/content/asthma_clips/valid_metadata.csv\")\n",
        "print(\"Loaded valid_metadata CSV\")\n",
        "print(valid_df.head())\n",
        "\n",
        "train_df = pd.read_csv(\"/content/asthma_clips/train_metadata.csv\")\n",
        "print(\"Loaded train_metadata CSV\")\n",
        "print(valid_df.head())\n"
      ],
      "metadata": {
        "id": "G3QhD2JZS29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df['original_label'].value_counts()"
      ],
      "metadata": {
        "id": "jHRV5EcYTpxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "def extract_hear_embedding(wav_path, model):\n",
        "    \"\"\"\n",
        "    Extracts a 512-dimensional audio embedding from a .wav file using a\n",
        "    TensorFlow-based model compatible with the HEAR (Holistic Evaluation of Audio Representations) benchmark.\n",
        "\n",
        "    This function:\n",
        "      - Loads a mono waveform from `wav_path` at the target sample rate `SR`\n",
        "      - Trims or pads the waveform to exactly `CLIP_LEN` samples (e.g., 2 seconds at 16kHz = 32000 samples)\n",
        "      - Feeds it into the model via its 'serving_default' signature\n",
        "      - Returns the resulting audio embedding as a 1D NumPy array\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    wav_path : str\n",
        "        Path to the audio file to process (.wav format).\n",
        "\n",
        "    model : tf.saved_model\n",
        "        A TensorFlow model loaded via `tf.saved_model.load(...)` that exposes\n",
        "        a 'serving_default' signature and returns an 'audio_embedding' tensor.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    np.ndarray\n",
        "        A 1D NumPy array of shape (512,) representing the audio embedding.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load audio file and resample to SR\n",
        "    y, _ = librosa.load(wav_path, sr=SR)\n",
        "\n",
        "    # Ensure the clip is exactly CLIP_LEN samples long\n",
        "    y = y[:CLIP_LEN]\n",
        "    if len(y) < CLIP_LEN:\n",
        "        y = np.pad(y, (0, CLIP_LEN - len(y)))\n",
        "\n",
        "    # Prepare as a batch of size 1 (shape: [1, CLIP_LEN])\n",
        "    batch = np.expand_dims(y, 0).astype(np.float32)\n",
        "\n",
        "    # Use the model's signature to get the embedding\n",
        "    embedding_tensor = model.signatures['serving_default'](tf.constant(batch))['audio_embedding']\n",
        "\n",
        "    # Convert to NumPy and return as 1D array\n",
        "    return embedding_tensor.numpy().squeeze()  # shape: (512,)\n"
      ],
      "metadata": {
        "id": "256jHUNKVW5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate embeddings\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "SR = 16000\n",
        "CLIP_LEN = 2 * SR\n",
        "BATCH_SIZE = 8  # tune based on GPU memory\n",
        "\n",
        "# Re‑load model under GPU\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "model = from_pretrained_keras(\"google/hear\")\n",
        "infer = model.signatures['serving_default']\n",
        "\n",
        "def load_clip(path):\n",
        "    \"\"\"\n",
        "    Loads an audio clip from the specified file path, resamples it to the target sample rate (SR),\n",
        "    and ensures it is exactly CLIP_LEN samples long by trimming or zero-padding as needed.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the .wav audio file.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 1D NumPy array of length CLIP_LEN representing the audio signal.\n",
        "    \"\"\"\n",
        "    y, _ = librosa.load(path, sr=SR)         # Load audio and resample to SR\n",
        "    y = y[:CLIP_LEN]                         # Trim to maximum allowed length\n",
        "    if len(y) < CLIP_LEN:\n",
        "        y = np.pad(y, (0, CLIP_LEN - len(y)))  # Pad with zeros if shorter than CLIP_LEN\n",
        "    return y\n",
        "\n",
        "\n",
        "def batched_generate(clip_dir, metadata_csv, out_prefix):\n",
        "    \"\"\"\n",
        "    Generates audio embeddings in batches from a directory of 2-second clips\n",
        "    and a corresponding metadata CSV.\n",
        "\n",
        "    This function:\n",
        "    - Loads audio clip paths and binary labels from the CSV\n",
        "    - Processes clips in batches using the preloaded `infer` model\n",
        "    - Extracts 512-dimensional embeddings for each clip\n",
        "    - Saves the embeddings, labels, and filenames to disk\n",
        "\n",
        "    Args:\n",
        "        clip_dir (str): Directory containing preprocessed 2-second .wav files.\n",
        "        metadata_csv (str): Path to a CSV file with columns ['filename', 'binary_label'].\n",
        "        out_prefix (str): Prefix for output files (without extension).\n",
        "\n",
        "    Outputs:\n",
        "        - {out_prefix}_embeddings.npy: Numpy array of shape (N, 512)\n",
        "        - {out_prefix}_labels.npy: Numpy array of shape (N,)\n",
        "        - {out_prefix}_filenames.txt: Text file with one filename per line\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(metadata_csv)  # Load metadata CSV\n",
        "    paths = [os.path.join(clip_dir, fn) for fn in df.filename]  # Full paths to each clip\n",
        "    labels = df.binary_label.values  # Corresponding binary labels\n",
        "\n",
        "    all_embs, all_labels, all_fns = [], [], []  # Lists to collect outputs\n",
        "\n",
        "    # Process clips in batches\n",
        "    for i in tqdm(range(0, len(paths), BATCH_SIZE), desc=out_prefix):\n",
        "        batch_paths = paths[i : i + BATCH_SIZE]  # Clip paths for current batch\n",
        "        batch_fns   = df.filename.values[i : i + BATCH_SIZE]  # Filenames for batch\n",
        "        batch_lbls  = labels[i : i + BATCH_SIZE]  # Labels for batch\n",
        "\n",
        "        # Load and stack waveforms\n",
        "        clips = np.stack([load_clip(p) for p in batch_paths]).astype(np.float32)\n",
        "\n",
        "        # Inference on GPU using HEAR model\n",
        "        out = infer(x=tf.constant(clips))['output_0'].numpy()\n",
        "\n",
        "        # Collect embeddings, labels, filenames\n",
        "        all_embs.append(out)\n",
        "        all_labels.append(batch_lbls)\n",
        "        all_fns.extend(batch_fns)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    X = np.vstack(all_embs)\n",
        "    y = np.concatenate(all_labels)\n",
        "\n",
        "    # Save to disk\n",
        "    np.save(f\"{out_prefix}_embeddings.npy\", X)\n",
        "    np.save(f\"{out_prefix}_labels.npy\", y)\n",
        "    with open(f\"{out_prefix}_filenames.txt\", \"w\") as f:\n",
        "        f.writelines(fn + \"\\n\" for fn in all_fns)\n",
        "\n",
        "    print(f\"Saved {X.shape[0]} embeddings as {out_prefix}_embeddings.npy\")\n",
        "\n",
        "\n",
        "# Run for train & valid\n",
        "batched_generate(\"/content/asthma_clips/train\", \"/content/asthma_clips/train_metadata.csv\", \"/content/train\")\n",
        "batched_generate(\"/content/asthma_clips/valid\", \"/content/asthma_clips/valid_metadata.csv\", \"/content/valid\")\n"
      ],
      "metadata": {
        "id": "7rEP87fNXmng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings to Google Drive\n",
        "\n",
        "!mkdir -p \"/content/drive/MyDrive/HeAR_Embeddings\"\n",
        "\n",
        "# Train files\n",
        "!cp /content/train_embeddings.npy /content/drive/MyDrive/HeAR_Embeddings/\n",
        "!cp /content/train_labels.npy /content/drive/MyDrive/HeAR_Embeddings/\n",
        "!cp /content/train_filenames.txt /content/drive/MyDrive/HeAR_Embeddings/\n",
        "\n",
        "# Validation files\n",
        "!cp /content/valid_embeddings.npy /content/drive/MyDrive/HeAR_Embeddings/\n",
        "!cp /content/valid_labels.npy /content/drive/MyDrive/HeAR_Embeddings/\n",
        "!cp /content/valid_filenames.txt /content/drive/MyDrive/HeAR_Embeddings/\n"
      ],
      "metadata": {
        "id": "kR-NrEThiJP6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}